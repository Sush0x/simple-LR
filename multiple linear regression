#Multiple Linear Regression
#Theory  - y(Profit) = b0 + b1*x1(R&D Spend) + b2*x2(Administration)+....+ bn*xn

#Data is about 50 startup companies investing in the different sectors to gain profit
#Idea is to determine which investment leads to high profit or may be which city is good to invest - "California" or "New York"

#Data -  R&D Spend | Administration | Marketing Spend | State      | Profit
#         165324.2    132456.7         459875.8         California   191226.9
------------------------------------------------------------------------------------------------------------------------------------------
#Intuition #1 - 
#Intuition #3 - 

#Intuition #4 - DUmmy varialble for categorical data
 | State   |             |  New York(D1) |  California(D2) |      
 California      }----->      0           1
 New York        }            1           0
 
# Dummy Variable Trap - D2 = D1 - 1  therefore we don't keep b0 , D1 and D2 at same time in equation hence remove one Dummy variable in the eqn.
-----------------------------------------------------------------------------------------------------------------------------------------
# p - value => used in null hypothesis 

u = assumed mean = 20 ounces
u' = calc. mean  = 22 ounces
sigma = variance , n = total number of observation

H0(Null hypothesis) => u = 20
H1(Alternate hypothesis) => u > 20

#p-value is calculated -
  z = (u' - u) / (sigma/sqrt(n))
  
 we calculate P(Z) - probibility at Z = <3>
 P = .0013 
 here P <0.05 therefore weak evidence hence we neglect the null hypothesis, hence u = 20 is not significant.
 -----------------------------------------------------------------------------------------------------------------------------------------
#5 methods of building model ----

1.all-in -- Use all (Used when u have prior knowlege | you have to use | preparing for Backward elimination )
2.Backward elimination --

*Select a significance level s= 0.05
*Fit the full model/ take all columns
*predict the p-value for each , if p-value > s go to step 4
*remove that column
*Fit model without these variable
*Keep doing for all variable whose p-value > s ie. 5%

3.Forward Selection
4.Bidirectional elimination
5.Score comparison
-----------------------------------------------------------------------------------------------------------------------------------------
# Importing Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#Importing datasets
dataset = pd.read_csv("C:/Users/Sushant Singh/Desktop/Udemy_ML/Machine Learning A-Z Template Folder/....../50_Startups.csv")
#Independent Variable
X = dataset.iloc[:,:-1].values #Include all columns except last one. 
#Dependent Variable
Y = dataset.iloc[:,4].values

----------------------------------------------------------------------------------------------------------------------------------------
#Encoding Categorical data -- Here we have 3 categories - New York | California | Floridia
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X = LabelEncoder()
X[:,3]= labelencoder_X.fit_transform(X[:,3])
onehotencoder = OneHotEncoder(categorical_features = [3]) 
X = onehotencoder.fit_transform(X).toarray()

----------------------------------------------------------------------------------------------------------------------------------------

#Avoiding the Dummy Variable Trap --

X = X[:,1:]
#We removed one Dummy Variable
----------------------------------------------------------------------------------------------------------------------------------------
#Spliting data into test, train sets

from sklearn.cross_validation import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=0)

----------------------------------------------------------------------------------------------------------------------------------------
# Fitting Multiple LR to training set

from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, X_train)

Y_pred = regressor.predict(X_test)

#take the two tables Y_test and Y_pred and observe if the given values and the predicted values are close or not.

-----------------------------------------------------------------------------------------------------------------------------------------



#Optional-- 

#Feature Scaling - Optional/Not necessary every time
'''from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.fit_transform(X_test)
sc_Y = StandardScaler()
Y_train = sc_Y.fit_transform(Y_train)'''

